
Course: Apache Kafka for Event-Driven Spring Boot Microservices

Start: 28/11/2025


****************************************
Section 1: Introduction to Apache Kafka
****************************************

- What is a Microservice?
    - Small autonomous application, that is usually designed to perform a one specific
      business functionality.
    - Responsible for specific functionality (Search, Email Notification, SMS Notification).
    - Loosely coupled, designed to scale and work in the cloud.

- Communication between Microservices.
    - It is performed by HTTP Requests in sync or async mode.

- Event-Driven Architecture: to send a message to differente applications at the same time.




********************************
Section 2: Apache Kafka Brokers
********************************


- Start single Apache Kafka broker with KRaft

    - docs: https://www.confluent.io/blog/set-up-and-run-kafka-on-windows-linux-wsl-2/

    - Clean up:
        
        rm -rf /tmp/kafka-logs /tmp/zookeeper /tmp/kraft-combined-logs

    - KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
    - echo $KAFKA_CLUSTER_ID
    - bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties
    - bin/kafka-server-start.sh config/kraft/server.properties


* Don't use this configuration to avoid issues.

- Multiple Kafka broker: Configuration Files (Multiple servers in a single cluster)
    - /config/kraft/
    - make copies of server.properties (server-1.properties, server-2.properties)
    - On server-2.properties
        - node.id=2
        - listeners=PLAINTEXT://:9094,CONTROLLER://:9095 
        - controller.quorum.voters=1@localhost:9093,2@localhost:9095  (do the same in server-1, the same entire value)
        - advertised.listeners=PLAINTEXT://localhost:9094
        - log.dirs=/tmp/server-2/kraft-combined-logs (do similar to server-1)

- Multiple Kafka broker: Storage folders
    - bin/kafka-storage.sh random-uuid
    - bin/kafka-storage.sh format -t dw6Kk_63Qrab_bN87YRAvg -c config/kraft/server-1.properties
    - bin/kafka-storage.sh format -t dw6Kk_63Qrab_bN87YRAvg -c config/kraft/server-2.properties   ---> is the same key as server-1


- Starting multiple Kafka broker with KRaft
    - bin/kafka-server-start.sh config/kraft/server-1.properties
    - bin/kafka-server-start.sh config/kraft/server-2.properties
    - see the logs, no errors, and the reference of the nodes.

- Stop Producers and Consumers
    - avoid losing messages and errors.
    - bin/kafka-server-stop.sh ---> graceful shutdown, better then control+c (2 servers down automatically)

-------------------------------------


*****************************
Section 3: Kafka CLI: Topics
*****************************

    - start both brokers in the cluster.

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

    - bin/kafka-topics.sh --bootstrap-server localhost:9092,localhost:9094 --create --topic topic1 --partitions 3 --replication-factor 2

        - Number of partitions should be the same of number of consumers.
        - replication factor: how many copies of each partition are store across of different servers. fault tolerance.
          data available even if a broker fails. this number cannot be greater than the number of brokers in the cluster.
    
    - bin/kafka-topics.sh --bootstrap-server localhost:9092,localhost:9094 --create --topic topic2 --partitions 3 --replication-factor 2

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic topic1



********************************
Section 4: Kafka CLI: Producers
********************************

    - Send a message with or without a Key.
    - Send multiple messages from a file.

    - Use only one server

    - Without a key

        - bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic topic3

            - if the topic doesn't exist it will be created automatically
        
    - Key:Value Pair

        - bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic topic4 --property "parse.key=true" --property "key.separator=:"

            - firstName:Julio
            - other format throws an error



********************************
Section 5: Kafka CLI: Consumers
********************************

    - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning
    - the message still alive in the topic after it was read
    - one or more consumers can see the messages from the topic

    - Consuming new messages only
        - enter the command without the --from-beginning option
    
    - Consuming Key:Value pair messages from Kafka topic
        
        - bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic my-topic --property "parse.key=true" --property "key.separator=:"

        - I need to add the print.key property, otherwise I just received the value

            - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning --property print.key=true

        - I also can print the value or not

            - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning --property print.key=true --property print.value=false

    - Consuming Kafka messages in order

        Messages with the same key are ordered in the same partition but the others are distributed across different partitions and there is no guarantee of order.

        - bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic messages-order --partitions 3 --replication-factor 2

        - bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic messages-order --property "parse.key=true" --property "key.separator=:"

            >1:message1
            >1:message2
            >1:message3
            >1:message4
            >1:message5
            >a:a
            >b:b
            >c:c
            >d:d
            >e:e
            >f:f
            >g:g

        - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic messages-order --from-beginning --property print.key=true --property print.value=true

            a       a
            c       c
            b       b
            d       d
            e       e
            1       message1
            1       message2
            1       message3
            1       message4
            1       message5
            f       f
            g       g            


****************************************************
Section 6: Kafka Producer - Spring Boot Microservice
****************************************************

branch: feature/section5-kafka-producer-microservice

    - Create new project
        - com.vilelo.ws
        - products-microservice
        - package: products
        - java 17
        - dependency: web, spring for kafka

    - add jackson dependency

    - configs on properties
        - server.port=0  ---> takes a random port
        - the correct key is just kafka server not producer server. 

    - create KafkaConfig, I don't use the replicas because I only have 1 server

    - test connection with the wsl: Test-NetConnection -ComputerName 172.30.84.55 -Port 9092

    - for the wsl use: ip addr show
        - use the eth0

    - edit server.properties with this:
        listeners=PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
        advertised.listeners=PLAINTEXT://172.30.84.55:9092

    - terminate the code (controller, service, config, model)
        - use join() if you want to wait confirmation from kafka

    - use a consumer:
        - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic product-created-events-topic --property print.key=true

    - test successful
        - kafka console: fbfd8ab3-55e4-4c91-98cd-df0eda942a87    {"productId":"fbfd8ab3-55e4-4c91-98cd-df0eda942a87","title":"iPhone 13","price":1700,"quantity":1}
        - even the logs are correct (async communication):
            - first : *********** Returning product id   (at the final of the method)
            - and then: *********** Message sent successfully


    - Kafka Producer: Send Message Synchronously

        - i tested future.join() is not working, the consumer didn't catch the message
        - edit serviceImpl using get()
        - edit the controller to handle the exception


---------------------------------------------------


*******************************************************
Section 7: Kafka Producer - Acknowledgements & Retries
*******************************************************


branch: feature/section7-kafka-producer-retries

    Acknowledgements:

    - spring.kafka.producer.acks=all    ---> waits for acknowledgements from all brokers. it ensures higher data durability by requiring acknowledgement from all
                                             in-sync replicas.

    - spring.kafka.producer.acks=1    ---> waits for acknowledgements from leader broker

        - the acknowledgement level required from the kafka cluster for successful message delivery
    
    - complement with:
        --replication-factor=5
        --config min.insync.replicas=2

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic insync-topic1 --partitions 3 --replication-factor 3 --config min.insync.replicas=2

    - bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --entity-type topics --entity-name topic2 --add-config min.insync.replicas=2

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe

    Retries:

    - spring.kafka.producer.retries=10   ---> times kafka will try to send a message
    - spring.kafka.producer.properties.retry.backoff.ms=1000  ---> how long the producer wil wait  before attempting to retry
    - spring.kafka.producer.properties.delivery.timeout.ms=120000 ---> (2minutes) max time producer can spend trying to deliver the message
    
    - spring.kafka.producer.properties.linger.ms=0   ---> max time in milliseconds that the producer will wait and buffer data before sending a batch of messages. default value is 0.
    - spring.kafka.producer.properties.request.timeout.ms=30000 ---> (30 secs) max time to wait for a response from the broker after sending a request. default value is 30000ms


    Example:

        - with the consumer in place with the insync property
        - first try this:
            - spring.kafka.producer.acks=all
            - start 3 servers and shut down one by one
            - when you have only one server active Kafka throws an exception
        - then:
            - add these:
                - spring.kafka.producer.retries=10
                - spring.kafka.producer.properties.retry.backoff.ms=1000
            - do the same as before
            - when you have only one server it will be retries every second
            - check the logs

    --------------

    Kafka Producer Delivery & Request Timeout
    Trying how Kafka Producer Delivery & Request Timeout works

        - comment the last two properties (retries and backoff.ms)
        - add these:
            - spring.kafka.producer.properties.delivery.timeout.ms=120000
            - spring.kafka.producer.properties.linger.ms=0
            - spring.kafka.producer.properties.request.timeout.ms=30000

        - also with 3 brokers and shut down one by one
        - when I have only one broker and send the request from postman
        - there will be multiple retries until the timeoutexception will be thrown

    ----------------

    Kafka Producer Spring Bean Configuration

        - Edit KafkaConfig adding producerCOnfigs(), producerFactory and kafkaTemplate methods
        - for now no need to add all properties like acks, deliveryTimeout, etc
        - just the necessaries as servers, key and value serializer
        - i test the new config with necessaries properties

            - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic product-created-events-topic --property print.key=true

        - "Invalid value {spring.kafka.producer.acks} for configuration acks: String must be one of: all, -1, 0, 1",
        - so in this case, acks property needs to be properly defined. I just commented that unnecessary code for now.
        - test successful



****************************************
Section 8: Idempotent Producer in Kafka
****************************************

branch: feature/section8-idempotent-kafka-producer

- Happy path:
    - a producer sends a message to the broker
    - the broker store the message in the topic
    - the broker send acknowledgement to the producer

- a possible escenario can be a network connection error taking place
  and the acknowledgement could not reach the producer.
    - so, the producer retry send the same message to the broker.
    - (*) the broker stores the message again.
    - the broker sends acknowledgement to the producer again and this time it was succesful.
    - the problem here is the duplicate same message in the kafka topic.
        - maybe not a big problem, but if it is a banking application it is a huge problem
        - products can be charged two times in a client account
        - to avoid this problem you need to make the producer IDEMPOTENT


Idempotent producer:
    - avoids duplicate message in the log in the presence of failure and retries.
    - it means that the producer can send the message multiple times but kafka broker will only store at once
      and keep it in the correct order.
    - this feature prevent inconsistency in the topic, even if there are problems with the network or server.
    - (*) with idempotent producer the message won't be store again in the topic. The broker just send the
       acknowledgement to the producer.
    
    - spring.kafka.producer.properties.enable.idempotence=true   ---> good idea to declare this. by default is active.

    - ConfigException
        - if I use acks=all there will by
        - spring.kafka.producer.properties.max.in.flight.request.per.connection=5    ---> items sent at the same time
    



*****************************************************
Section 9: Kafka Consumer - Spring Boot Microservice
*****************************************************

branch: feature/section9-kafka-consumer

the topic has a specified number of partitions and each partition can be readed by one consumer.

- New service to read a message from a topic.
- when a spring boot application read a message from the topic that message still remains in the topic. until it will be deleted automatically (168hrs).
- so, it is possible to have multiple and different consumers like different applications. SMS Notification, Email Notification, Push Notification microservices.

--------------------

- Create new project
    - com.vilelo.ws
    - email-notification-microservice
    - package: com.vilelo.ws.emailnotification
    - java 17
    - dependency: web, spring for kafka, lombok

    - add jackson dependency  ---> to avoid a KafkaException

--------------------

- Configs
    - properties
        - trusted.packages=*   ---> to accept any package
        - I don't like tu use JsonDeserializer because is deprecated

    - listener
        - @KafkaListener accepts multiple topics

--------------------

Create the core project
    - com.vilelo.ws
    - core
    - package: com.vilelo.ws.core
    - java 17
    - no dependency

Config:
    - delete build section of the pom
    - delete dependencies of the pom
    - delete main app file and the test
    - create the ProductCreatedEvent class
    - copy the three important values to import as a depedency in other project
        
        <groupId>com.vilelo.ws</groupId>
	    <artifactId>core</artifactId>
	    <version>0.0.1-SNAPSHOT</version>
    
    - mvn clean install  --> core
    - add as a dependency in products-microservice
    - do the same to email-notification-microservice

------------------------

@KafkaHandler: Trying how it works

    - run products-microservice
    - run email-notification-microservice
    - the test was succesful. I use product service as a producer
      and the message was received by email-notification-service.

------------------------

ConsumerFactory Bean Configuration
Kafka Listener Container Factory

    - i don't use ther spring Value annotation to get a value from properties file
    - i use Environment class to get values, delete unnecessary properties
    - i also use jacksonjsondeserializer as an updated alternative for DESERIALIZATION

    - ConsumerFactory   --> provides the necessary configuration
    - ConcurrentKafkaListenerContainerFactory  ---> Create a Kafka container and a kafka listener
    - the test was succesful with the new configurations