
Course: Apache Kafka for Event-Driven Spring Boot Microservices

Start: 28/11/2025


****************************************
Section 1: Introduction to Apache Kafka
****************************************

- What is a Microservice?
    - Small autonomous application, that is usually designed to perform a one specific
      business functionality.
    - Responsible for specific functionality (Search, Email Notification, SMS Notification).
    - Loosely coupled, designed to scale and work in the cloud.

- Communication between Microservices.
    - It is performed by HTTP Requests in sync or async mode.

- Event-Driven Architecture: to send a message to differente applications at the same time.




********************************
Section 2: Apache Kafka Brokers
********************************


- Start single Apache Kafka broker with KRaft

    - docs: https://www.confluent.io/blog/set-up-and-run-kafka-on-windows-linux-wsl-2/

    - Clean up:
        
        rm -rf /tmp/kafka-logs /tmp/zookeeper /tmp/kraft-combined-logs

    - KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
    - echo $KAFKA_CLUSTER_ID
    - bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties
    - bin/kafka-server-start.sh config/kraft/server.properties


* Don't use this configuration to avoid issues.

- Multiple Kafka broker: Configuration Files (Multiple servers in a single cluster)
    - /config/kraft/
    - make copies of server.properties (server-1.properties, server-2.properties)
    - On server-2.properties
        - node.id=2
        - listeners=PLAINTEXT://:9094,CONTROLLER://:9095 
        - controller.quorum.voters=1@localhost:9093,2@localhost:9095  (do the same in server-1, the same entire value)
        - advertised.listeners=PLAINTEXT://localhost:9094
        - log.dirs=/tmp/server-2/kraft-combined-logs (do similar to server-1)

- Multiple Kafka broker: Storage folders
    - bin/kafka-storage.sh random-uuid
    - bin/kafka-storage.sh format -t dw6Kk_63Qrab_bN87YRAvg -c config/kraft/server-1.properties
    - bin/kafka-storage.sh format -t dw6Kk_63Qrab_bN87YRAvg -c config/kraft/server-2.properties   ---> is the same key as server-1


- Starting multiple Kafka broker with KRaft
    - bin/kafka-server-start.sh config/kraft/server-1.properties
    - bin/kafka-server-start.sh config/kraft/server-2.properties
    - see the logs, no errors, and the reference of the nodes.

- Stop Producers and Consumers
    - avoid losing messages and errors.
    - bin/kafka-server-stop.sh ---> graceful shutdown, better then control+c (2 servers down automatically)

-------------------------------------


*****************************
Section 3: Kafka CLI: Topics
*****************************

    - start both brokers in the cluster.

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

    - bin/kafka-topics.sh --bootstrap-server localhost:9092,localhost:9094 --create --topic topic1 --partitions 3 --replication-factor 2

        - Number of partitions should be the same of number of consumers.
        - replication factor: how many copies of each partition are store across of different servers. fault tolerance.
          data available even if a broker fails. this number cannot be greater than the number of brokers in the cluster.
    
    - bin/kafka-topics.sh --bootstrap-server localhost:9092,localhost:9094 --create --topic topic2 --partitions 3 --replication-factor 2

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic topic1



********************************
Section 4: Kafka CLI: Producers
********************************

    - Send a message with or without a Key.
    - Send multiple messages from a file.

    - Use only one server

    - Without a key

        - bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic topic3

            - if the topic doesn't exist it will be created automatically
        
    - Key:Value Pair

        - bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic topic4 --property "parse.key=true" --property "key.separator=:"

            - firstName:Julio
            - other format throws an error



********************************
Section 5: Kafka CLI: Consumers
********************************

    - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning
    - the message still alive in the topic after it was read
    - one or more consumers can see the messages from the topic

    - Consuming new messages only
        - enter the command without the --from-beginning option
    
    - Consuming Key:Value pair messages from Kafka topic
        
        - bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic my-topic --property "parse.key=true" --property "key.separator=:"

        - I need to add the print.key property, otherwise I just received the value

            - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning --property print.key=true

        - I also can print the value or not

            - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning --property print.key=true --property print.value=false

    - Consuming Kafka messages in order

        Messages with the same key are ordered in the same partition but the others are distributed across different partitions and there is no guarantee of order.

        - bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic messages-order --partitions 3 --replication-factor 2

        - bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic messages-order --property "parse.key=true" --property "key.separator=:"

            >1:message1
            >1:message2
            >1:message3
            >1:message4
            >1:message5
            >a:a
            >b:b
            >c:c
            >d:d
            >e:e
            >f:f
            >g:g

        - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic messages-order --from-beginning --property print.key=true --property print.value=true

            a       a
            c       c
            b       b
            d       d
            e       e
            1       message1
            1       message2
            1       message3
            1       message4
            1       message5
            f       f
            g       g            


****************************************************
Section 6: Kafka Producer - Spring Boot Microservice
****************************************************

branch: feature/section5-kafka-producer-microservice

    - Create new project
        - com.vilelo.ws
        - products-microservice
        - package: products
        - java 17
        - dependency: web, spring for kafka

    - add jackson dependency

    - configs on properties
        - server.port=0  ---> takes a random port
        - the correct key is just kafka server not producer server. 

    - create KafkaConfig, I don't use the replicas because I only have 1 server

    - test connection with the wsl: Test-NetConnection -ComputerName 172.30.84.55 -Port 9092

    - for the wsl use: ip addr show
        - use the eth0

    - edit server.properties with this:
        listeners=PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
        advertised.listeners=PLAINTEXT://172.30.84.55:9092

    - terminate the code (controller, service, config, model)
        - use join() if you want to wait confirmation from kafka

    - use a consumer:
        - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic product-created-events-topic --property print.key=true

    - test successful
        - kafka console: fbfd8ab3-55e4-4c91-98cd-df0eda942a87    {"productId":"fbfd8ab3-55e4-4c91-98cd-df0eda942a87","title":"iPhone 13","price":1700,"quantity":1}
        - even the logs are correct (async communication):
            - first : *********** Returning product id   (at the final of the method)
            - and then: *********** Message sent successfully


    - Kafka Producer: Send Message Synchronously

        - i tested future.join() is not working, the consumer didn't catch the message
        - edit serviceImpl using get()
        - edit the controller to handle the exception


---------------------------------------------------


*******************************************************
Section 7: Kafka Producer - Acknowledgements & Retries
*******************************************************


branch: feature/section7-kafka-producer-retries

    Acknowledgements:

    - spring.kafka.producer.acks=all    ---> waits for acknowledgements from all brokers. it ensures higher data durability by requiring acknowledgement from all
                                             in-sync replicas.

    - spring.kafka.producer.acks=1    ---> waits for acknowledgements from leader broker

        - the acknowledgement level required from the kafka cluster for successful message delivery
    
    - complement with:
        --replication-factor=5
        --config min.insync.replicas=2

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic insync-topic1 --partitions 3 --replication-factor 3 --config min.insync.replicas=2

    - bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --entity-type topics --entity-name topic2 --add-config min.insync.replicas=2

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe

    Retries:

    - spring.kafka.producer.retries=10   ---> times kafka will try to send a message
    - spring.kafka.producer.properties.retry.backoff.ms=1000  ---> how long the producer wil wait  before attempting to retry
    - spring.kafka.producer.properties.delivery.timeout.ms=120000 ---> (2minutes) max time producer can spend trying to deliver the message
    
    - spring.kafka.producer.properties.linger.ms=0   ---> max time in milliseconds that the producer will wait and buffer data before sending a batch of messages. default value is 0.
    - spring.kafka.producer.properties.request.timeout.ms=30000 ---> (30 secs) max time to wait for a response from the broker after sending a request. default value is 30000ms


    Example:

        - with the consumer in place with the insync property
        - first try this:
            - spring.kafka.producer.acks=all
            - start 3 servers and shut down one by one
            - when you have only one server active Kafka throws an exception
        - then:
            - add these:
                - spring.kafka.producer.retries=10
                - spring.kafka.producer.properties.retry.backoff.ms=1000
            - do the same as before
            - when you have only one server it will be retries every second
            - check the logs

    --------------

    Kafka Producer Delivery & Request Timeout
    Trying how Kafka Producer Delivery & Request Timeout works

        - comment the last two properties (retries and backoff.ms)
        - add these:
            - spring.kafka.producer.properties.delivery.timeout.ms=120000
            - spring.kafka.producer.properties.linger.ms=0
            - spring.kafka.producer.properties.request.timeout.ms=30000

        - also with 3 brokers and shut down one by one
        - when I have only one broker and send the request from postman
        - there will be multiple retries until the timeoutexception will be thrown

    ----------------

    Kafka Producer Spring Bean Configuration

        - Edit KafkaConfig adding producerCOnfigs(), producerFactory and kafkaTemplate methods
        - for now no need to add all properties like acks, deliveryTimeout, etc
        - just the necessaries as servers, key and value serializer
        - i test the new config with necessaries properties

            - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic product-created-events-topic --property print.key=true

        - "Invalid value {spring.kafka.producer.acks} for configuration acks: String must be one of: all, -1, 0, 1",
        - so in this case, acks property needs to be properly defined. I just commented that unnecessary code for now.
        - test successful



****************************************
Section 8: Idempotent Producer in Kafka
****************************************

branch: feature/section8-idempotent-kafka-producer

- Happy path:
    - a producer sends a message to the broker
    - the broker store the message in the topic
    - the broker send acknowledgement to the producer

- a possible escenario can be a network connection error taking place
  and the acknowledgement could not reach the producer.
    - so, the producer retry send the same message to the broker.
    - (*) the broker stores the message again.
    - the broker sends acknowledgement to the producer again and this time it was succesful.
    - the problem here is the duplicate same message in the kafka topic.
        - maybe not a big problem, but if it is a banking application it is a huge problem
        - products can be charged two times in a client account
        - to avoid this problem you need to make the producer IDEMPOTENT


Idempotent producer:
    - avoids duplicate message in the log in the presence of failure and retries.
    - it means that the producer can send the message multiple times but kafka broker will only store at once
      and keep it in the correct order.
    - this feature prevent inconsistency in the topic, even if there are problems with the network or server.
    - (*) with idempotent producer the message won't be store again in the topic. The broker just send the
       acknowledgement to the producer.
    
    - spring.kafka.producer.properties.enable.idempotence=true   ---> good idea to declare this. by default is active.

    - ConfigException
        - if I use acks=all there will by
        - spring.kafka.producer.properties.max.in.flight.request.per.connection=5    ---> items sent at the same time
    



*****************************************************
Section 9: Kafka Consumer - Spring Boot Microservice
*****************************************************

branch: feature/section9-kafka-consumer

the topic has a specified number of partitions and each partition can be readed by one consumer.

- New service to read a message from a topic.
- when a spring boot application read a message from the topic that message still remains in the topic. until it will be deleted automatically (168hrs).
- so, it is possible to have multiple and different consumers like different applications. SMS Notification, Email Notification, Push Notification microservices.

--------------------

- Create new project
    - com.vilelo.ws
    - email-notification-microservice
    - package: com.vilelo.ws.emailnotification
    - java 17
    - dependency: web, spring for kafka, lombok

    - add jackson dependency  ---> to avoid a KafkaException

--------------------

- Configs
    - properties
        - trusted.packages=*   ---> to accept any package
        - I don't like tu use JsonDeserializer because is deprecated

    - listener
        - @KafkaListener accepts multiple topics

--------------------

Create the core project
    - com.vilelo.ws
    - core
    - package: com.vilelo.ws.core
    - java 17
    - no dependency

Config:
    - delete build section of the pom
    - delete dependencies of the pom
    - delete main app file and the test
    - create the ProductCreatedEvent class
    - copy the three important values to import as a depedency in other project
        
        <groupId>com.vilelo.ws</groupId>
	    <artifactId>core</artifactId>
	    <version>0.0.1-SNAPSHOT</version>
    
    - mvn clean install  --> core
    - add as a dependency in products-microservice
    - do the same to email-notification-microservice

------------------------

@KafkaHandler: Trying how it works

    - run products-microservice
    - run email-notification-microservice
    - the test was succesful. I use product service as a producer
      and the message was received by email-notification-service.

------------------------

ConsumerFactory Bean Configuration
Kafka Listener Container Factory

    - i don't use ther spring Value annotation to get a value from properties file
    - i use Environment class to get values, delete unnecessary properties
    - i also use jacksonjsondeserializer as an updated alternative for DESERIALIZATION

    - ConsumerFactory   --> provides the necessary configuration
    - ConcurrentKafkaListenerContainerFactory  ---> Create a Kafka container and a kafka listener
    - the test was succesful with the new configurations


*******************************************************
Section 10: Kafka Consumer - Handle Deserializer Errors
*******************************************************

branch: feature/section9-kafka-consumer-handle-deserializer-errors

    - If I have a JSON Deserializer in my email-notification-service, then If I received a String Serializer
      there will be an error and my consumer is not prepared to handle this.

    - Causing the problem:

        - start the mail service
        - start a producer with the console
            - bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic product-created-events-topic --property "parse.key=true" --property "key.separator=:"
        - send an invalid message (not json format)
            - hola:mundox
        - the application is stuck, making impossible to handle other messages
          this is the poison pill effect.
        
    - Kafka Consumer ErrorHandlingDeserializer

        - Edit the consumer configuration by using ErrorHandlingDeserializer. the fields(2) were added.
        - when I start the app again, the error was thrown only one time, without send another message.
        - I tested this sending invalid message from the console and a correct message from the products-service
          and the exception was thrown only one time, it means the service handle it correctly.

-----------------------------------------

*******************************************************
Section 11: Kafka Consumer - Dead Letter Topic (DLT)
*******************************************************

branch: feature/section11-kafka-consumer-dlt

DLT:

- if the service receive an invalid message, then it can be redirect to another topic
that can be consumed as bad messages to analyze those messages.

- edit the consumer configuration specifically the kafkaListenerContainerFactory method

- there is no dlt name specified. the default value is the same as the kafka listener definition + -dlt

- Configure kafkaTemplate as a bean in consumer configuration class
- also configure producerFactory as a bean.
- I used classes from jackson because jsonserialzer is deprecated.

- start both products and emai-notification services

- bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic product-created-events-topic --property "parse.key=true" --property "key.separator=:"
- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic product-created-events-topic-dlt --from-beginning

- send correct json format from product-service
- send bad message from the producer console as this:  1:{juliochacon}

- base64 decode(go to the web browser to do this operation). take the value inside ""

- the test was successful


****************************************************
Section 12: Kafka Consumer - Exceptions and Retries
****************************************************

branch: feature/section12-kafka-consumer-exceptions-retries


- if the bad message is not retryable then the message will be send to the DLT
- but if the message is retryable then consume message again. that is possible with some configurations in the email-notif app.
- if exception is Retryable, then:
    - configure wait time
    - number of times to retry

- Creating retryable and not retryable exceptions in Kafka
    - create exception classes
    - edit consumer configuration
    - test NotRetryableException
        - edit ProductCreatedEventHandler. add a simple conditional to throw the NotRetryableException
        - bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic product-created-events-topic-dlt --from-beginning
        - test successful , the message was received in the dlt topic

    - test retryableException
        - new parameter in the error handler: new FixedBackOff(interval:5000, maxAttempts: 3)
        - invoke rest service, add new bean in main application class
        - add the restTemplate in ProductCreatedEventHandler
        - use the mock-service http://localhost:8085/response/200 or http://localhost:8085/response/500
        - the retryable 3 times in 5 seconds was successful. send the request with product-service and with mock service offline.
        - tests were successful

    
********************************************************************
Section 13: Kafka Consumer - Multiple Consumers in a Consumer Group
********************************************************************

branch: feature/section13-kafka-consumer-multiple-consumers-cg

Intro to Kafka Consumer Groups:

- I have three consumers listening to a topic (Email, SMS, Push notification services)
- The producer sends a message to the topic and that specific message is received by each service.
- Imagine that I need to scale up to three instances of Email Notification Service. The message emitted by the topic will be processed
  three times? One time for each instance?
- The answer is no, If I group the specific instances of the service in a "Consumer Group" then the message will be processed
  only one time by one specific instance.

- Kafka sends the message to only one consumer in the consumer group.

-------------

Rebalancing and Partition Assignment in Apache Kafka

    - If my topic has three partitions and my Consumer Group has four consumers(four service instances)
      the instance number 4 will be idle. I cannot have more consumers than number of partitions, because the extra consumer(s) will be idle.

    -  Every consumer in the consumer-group sends a heart beat or a signal every 3 secs by default and can be configured.
       This signal lets know to the Kafka broker that can consume new messages.

    - If a consumer stops sending signals to the broker, then the broker delete it from the consumer group and start the rebalancing.
      This means that the rest of available consumers in the consumer-group will consume the messages of all the partitions in the topic.

-------------

Assigning Microservice to a consumer group in Kafka

    - the configuration was already set in the properties file and ConsumerConfiguration class.
    - change to this in both files: consumer.group-id=product-created-events 

-------------


Starting up more Microservices
Trying how partitions assignment works in Kafka

    - set server.port=0 to get a random port
    - start 3 instances of email notif service
    - when you start the instances one by one you can see the rebalancing in the application logs
    - kafka assign three topics to the first available instance
    - then kafka rebalance the topics to the next available instance and so on.

        KafkaMessageListenerContainer: product-created-events:
            partitions assigned: [
                product-created-events-topic-0,
                product-created-events-topic-1,
                product-created-events-topic-2
            ]

-------------

Multiple consumers consuming messages from Kafka topic

    - in the handler add producId to the logs
    - start all the services and see the behavior of kafka and the processing of the messages in the consumer-group
    - mvn spring-boot:run

    - to check resources in kafka before and during the tests:

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

    - bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
        product-created-events

    - bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe
        - shows the product-created-events-topic with three partitions set by the product-microservice
    
    - send a request with product-microservice 

    - the message was processed only by one instance:

        product-created-events: partitions assigned: [product-created-events-topic-0]
        Received a new event: iPad pro 15 with productId: ea9feea8-60c6-44b7-98ce-ef42ed8f0c03

    - the test was succesful

-------------- 

*************************************************
Section 14: Kafka Consumer - Idempotent Consumer
*************************************************

branch: feature/section14-kafka-idempotent-consumer

- Can process the same message multiple times without causing any side effects or data inconsistencies.
- avoiding duplicate messages
    - idempotent consumer
    - idempotent producer
    - transactions: all complete or nothing

- Include a unique id into message header
    - Edit ProductServiImpl adding ProducerRecord into createProduct()

- Reading a unique id from message header
    - Edit ProductCreatedEventHandler adding headers
    - Adding database-related dependencies


- Adding database-related dependencies
    - add maven dependencies for h2 and jpa
    - add connection properties

- Create JPA Entity, Repository in package io
- edit ProductCreatedEventHandler adding the save of the object into the database
- mark with @Transactional because it made changes to the database. The use of this annotation
  means that if it happens an error in the method every change made to the database will be undone with a rollback. 

- Check if Kafka message was processed earlier
    - add validation in the handler with the new repository method

- Test the development:
    - harcode the messageId in ProductServiceImpl
    - send a request with product-service
    - go to the h2 console:  localhost:8081/h2-console
        - org.h2.Driver
        - jdbc:h2:mem:testdb
        - julio:julio
    - check the records of the table
    - send the same request
    - the record has not impact in the database because already in the table
    - then, comment the hardcode and use the dynamic messageId
    -  h2 web console doesn't work but the validation is correct
        - first with hardcoded value, the console show this: The event with id: 123 has already been processed
        - then with random value in messageId: 
    - as an alternative to see records on h2:
        - mvn dependency:copy-dependencies
        - java -cp .\target\dependency\h2-2.4.240.jar org.h2.tools.Shell
            - url: jdbc:h2:mem:testdb
            - driver: org.h2.Driver
            - username: julio
            - password: julio

        - It is necessary more investigation to query the correct db becasue until here is empty, no table created.


****************************************
Section 15: Apache Kafka - Transactions 
****************************************

branch: feature/section15-kafka-transactions

- All or nothing: all operations in kafka succeed or none of all take effect
- exactly once: we want the kafka message deliver once and only once
    - idempotent producer: duplicate messages are not sent
    - transactions: ensures that consumer does not read message that is part of incomplete transaction

    transfer.topic => transfer-service => withdrawals.topic => withdraw-service
                       (consumer)      => deposit.topic     => deposit-service
                       (producer)

* problem of 2 withdraws and one deposit. example: i want to transfer 50 dollars. the application withdraw 50 + 50 dollars of my acc and deposit 50.

- Imagine a transfer-service. Sometimes, this kind of service acts as both: consumer and producer
- The service received a transfer message from transfer.topic and send the message to withdrawals.topic
- there is another withdraw-service that consumes the message from withdrawals.topic and will withdraw the requested amount of money from user's acc.
- the transfer-service failed during this process. So, to kafka(transfers.topic broker) the message is not considered as successfully consumed.
- in this case, a new message is produce from the beginning, so, withdrawals.topic has two messages.
- transfer-service will then produce another message to a different topic (deposits.topic) and deposit-microservice process this message
- we want to avoid situations like this

- With transactions we have this:
    - We enable Kafka transactions on transfer-service so the following operations: withdrawals and deposits
      will executed within one single transaction.
    - the consumer services (withdraw and deposit) are configured to read only "commited messages"
    - during the process the message is marked as "uncommited". the consumers not see this message.
    - if an error ocurred with transfer-microservice the transaction is aborted and the message remains as "uncommited" in the topic
    - the second time, a new message is send to deposits.topic also as "uncommited". If the method complete successfully
      the messages are marked as "commited" and are ready to be consumed by consumer services.

* Transactions help us write to multiple topics atomically and help us achieve all or nothing behavior.
  If we need to write to multiple topics, then it is either all writes are successful or none of them are.

  Note: there is another concept "Compensating transactions". Allows us to rollback or undo operations that span multiple remote services.
  for example: if transfer-service need to write to a database outsite the transaction process, it will be a separate transaction.
  
  * Kafka doesn't manage database transactions.
  * Spring provides good integration for manage kafka and database transactions. This integrations will help us to rollback both:
    Kafka and database transactions.

--------------------------

Setup different microservices

    consumers: withdrawals and deposit services.

    * transfer-service
        spring.kafka.producer.transaction-id-prefix=transfer-service-${random.value}-
        logging.level.org.springframework.kafka.transaction=TRACE
        logging.level.org.springframework.transaction=TRACE

        - new prop in producer config TRANSACTIONAL_ID_CONFIG
        - @Transactional in ServiceImpl for the integration between spring and kafka
            - it is possible to specify the manager of the transaction component
            - in this case if the callRemoteService fails, then the consumer services don't receive the messages

--------------------------

Rollback transaction for specific exception

    - @Transactional performs a rollback when ocurrs Unchecked exceptions(RuntimeExceptions) and error(by default). That means no rollback for any kind of exception.
    - No rollback for Checked exceptions
    - it can be configure like this:
        - rollbackFor = Throwable.class   ---> something very general
        - rollbackFor = {TransferServiceException.class, ConnectException.class}  ---> more specific
        - noRollbackFor = {IndexOutOfBoundsException.class}
    - this specific configuration doesn't override the default configuration (RuntimeException and Errors). It is just to make sure to perform a rollback in these cases.
    - if there is only one transaction manager it is ok to have the default configurations, just @Transactional and nothing more

--------------------------

Reading committed messages in Kafka Consumer

    - in consumer services set this prop:
        - spring.kafka.consumer.isolation-level=READ_COMMITTED

    - set the isolation prop in KafkaConsumerConfig

--------------------------

Test the development:

    - start kafka server (3 recommended, I use just one)
    - start services, including mock services
    - send a request with transfer service: POST http://localhost:8086/transfers
    - body:
        {
            "senderId": "123",
            "recepientId": "124",
            "amount": 5589
        }
    - test success case [OK]
    - test connection refused with mock service off [OK]
    - test when mock service returns and exception(invoke with /500) [OK]
    - check the logs with "KafkaTransactionManager" [OK]

    - tests successful

--------------------------








